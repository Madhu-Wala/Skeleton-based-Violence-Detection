# Skeleton-based-Violence-Detection

## ğŸ“Œ Project Overview

This project implements a pose-based violence detection system using human skeleton keypoints extracted from videos and a Bidirectional LSTM (BiLSTM) neural network for temporal classification.

Instead of raw RGB video frames, the model operates on human pose sequences, making it computationally efficient and privacy-preserving.

## ğŸ—‚ï¸ Project Folder Structure (Original Design)

```
Skeleton-based-Violence-Detection/
â”‚
â”œâ”€â”€ dataset/ 
â”‚   â””â”€â”€ RWF2000/ 
â”‚       â”œâ”€â”€ train/
â”‚       â”‚   â”œâ”€â”€ Fight/
â”‚       â”‚   â””â”€â”€ NonFight/
â”‚       â””â”€â”€ val/
â”‚           â”œâ”€â”€ Fight/
â”‚           â””â”€â”€ NonFight/
â”‚
â”œâ”€â”€ extracted_keypoints/ 
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ Fight/
â”‚   â”‚   â””â”€â”€ NonFight/
â”‚   â””â”€â”€ val/
â”‚       â”œâ”€â”€ Fight/
â”‚       â””â”€â”€ NonFight/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pose_extractor.py
â”‚   â”œâ”€â”€ dataset_loader.py
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ predict.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_keypoints.py
â”‚   â”œâ”€â”€ train_bilstm.py
â”‚   â”œâ”€â”€ infer_video.py
â”‚   â”œâ”€â”€ evaluate_model.py
â”‚   â””â”€â”€ plot_metrics.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ bilstm_model_165.h5
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---
## ğŸ¯ Objective

To classify videos into:

- Fight (Violence)
- NonFight (Non-Violence)

using temporal pose information extracted from video frames.

---
## ğŸ”§ Technologies Used

- Python 3.12
- MediaPipe Pose
- OpenCV
- TensorFlow / Keras
- NumPy
- Matplotlib
- Scikit-learn

---
## ğŸ” Methodology
### 1ï¸âƒ£ Pose Extraction

- Each video is processed frame-by-frame
- MediaPipe Pose extracts 33 body landmarks
- Each landmark contributes: ``` x, y, visibility```
- Total features per frame = ``` 33 Ã— 3 = 99```
- Saved as: ```(frames, 99) â†’ .npy files```

### 2ï¸âƒ£ Sequence Normalization

- Variable-length sequences are:
- Padded
- Truncated
- Final input shape:
```
(samples, 30 frames, 99 features)
```

### 3ï¸âƒ£ Model Architecture

#### Bidirectional LSTM (BiLSTM)
```
Input (30 Ã— 99)
 â†’ Masking
 â†’ BiLSTM (128 units)
 â†’ Dropout
 â†’ BiLSTM (64 units)
 â†’ Dropout
 â†’ Dense (ReLU)
 â†’ Dense (Sigmoid)
```

Loss: Binary Crossentropy

Optimizer: Adam

Metric: Accuracy

ğŸ“ˆ Observations

Accuracy: 0.6268221574344023

 Classification Report:
 ```
               precision    recall  f1-score   support

     NonFight       0.64      0.42      0.51       157
        Fight       0.62      0.80      0.70       186

     accuracy                           0.63       343
    macro avg       0.63      0.61      0.60       343
 weighted avg       0.63      0.63      0.61       343
```

Confusion Matrix: 

<img width="402" height="387" alt="image" src="https://github.com/user-attachments/assets/328b29f3-d9bb-4913-a1b5-0675812e7e8d" />

ROC Curve:

<img width="402" height="387" alt="image" src="https://github.com/user-attachments/assets/8583972a-9eed-4267-9b9d-69e23e9e0112" />

Prediction Confidence Distribution:

<img width="402" height="387" alt="image" src="https://github.com/user-attachments/assets/b28344de-8f9e-4a97-83a0-7fc1cb375d16" />

## â–¶ï¸ How to Run the Project
### 1ï¸âƒ£ Clone the Repository
```
git clone https://github.com/Madhu-Wala/Skeleton-based-Violence-Detection.git
cd Skeleton-based-Violence-Detection
```

### 2ï¸âƒ£ Create & Activate Virtual Environment
```
python -m venv venv
venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```

#### âš ï¸ Important Notes

TensorFlow runs on CPU (Windows limitation)

Dataset is not included (size â‰ˆ 12GB)

### 4ï¸âƒ£ Download Dataset Manually

Download RWF-2000 dataset from Kaggle:

ğŸ”— [https://www.kaggle.com/datasets/vulamnguyen/rwf2000](https://www.kaggle.com/datasets/vulamnguyen/rwf2000)

Extract and place it as:
```
dataset/
â””â”€â”€ RWF2000/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ Fight/
    â”‚   â””â”€â”€ NonFight/
    â””â”€â”€ val/
        â”œâ”€â”€ Fight/
        â””â”€â”€ NonFight/
```

### 5ï¸âƒ£ Extract Pose Keypoints

âš ï¸ This is a heavy step and may take 3-4 hours on CPU.
```
python scripts/extract_keypoints.py
```

This generates:
```
extracted_keypoints/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ Fight/
â”‚   â””â”€â”€ NonFight/
â””â”€â”€ val/
    â”œâ”€â”€ Fight/
    â””â”€â”€ NonFight/
```

Each .npy file contains:
```
(frames, 99)
```

### 6ï¸âƒ£ Train BiLSTM Model
```
python scripts/train_bilstm.py
```

Outputs:
```
models/bilstm_model_165.h5
```

Training time: ~10â€“20 minutes (CPU)

Accuracy ~55â€“60%

### 7ï¸âƒ£ Plot Performance Metrics
```
python scripts/plot_metrics.py
```

### 8ï¸âƒ£ Run Inference on a Video

Edit video path inside:
```
scripts/infer_video.py
```

Then run:
```
python scripts/infer_video.py
```

Output:
```
Prediction: Fight / NonFight
Confidence: 0.xx
```

## âš ï¸ Common Issues
### âŒ Low Accuracy (~0.55)

- âœ” Expected for pose-only approach
- âœ” Dataset contains ambiguous actions

### âŒ Some videos skipped during extraction

âœ” MediaPipe fails on:
- Corrupt videos
- No detectable human pose
- Occlusions

